{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Data Mining the Bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from wordcloud import WordCloud\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bibleoriginal = pd.read_csv(\"http://www.webpages.uidaho.edu/~stevel/Datasets/bible_asv.csv\")\n",
    "bibleoriginal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible = bibleoriginal\n",
    "#drop features that are not text or book\n",
    "bible = bible.drop(['field','Chapters','Verses'], axis=1)\n",
    "bible['text'] = bible['text'].str.lower()\n",
    "bible['text'] = bible['text'].str.replace('[^\\w\\s]','')\n",
    "bible['text'] = bible['text'].str.replace('\\d+', '')\n",
    "bible['text'] = bible['text'].str.strip()\n",
    "biblebooks = bible.groupby('Books').agg(' '.join)\n",
    "#tf-idf\n",
    "#first we create vector using only 500 words and removing useless stop words\n",
    "#we just consider words that matter to the target variable (salary)\n",
    "vector = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "data = pd.DataFrame(vector.fit_transform(biblebooks.text).toarray(), columns = vector.get_feature_names())\n",
    "#print(biblebooks.shape)\n",
    "#print(bibleVectorized.shape)\n",
    "#data = pd.concat(biblebooks['Testements'], biblebooks['Sections'], axis=1)\n",
    "#data.head()\n",
    "biblebooks['Sections'] = [n.partition(' ')[0] for n in biblebooks['Sections']]\n",
    "biblebooks['Testaments'] = [n.partition(' ')[0] for n in biblebooks['Testaments']]\n",
    "#biblebooks = pd.concat([biblebooks['Books'], bibleVectorized], axis=1)\n",
    "#print(biblebooks.shape)\n",
    "#biblebooks = biblebooks.drop('text', axis=1)\n",
    "#print (biblebooks.shape)\n",
    "#print (data.shape)\n",
    "#print(biblebooks['text'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(11, 6, figsize=(20, 20))\n",
    "for a in range(11):\n",
    "    for b in range(6):\n",
    "        wordcloud = WordCloud(max_words=100,background_color=\"#927c5c\").generate(biblebooks['text'][b*11+a])\n",
    "        ax[a,b].set(title='k-means Clustering')\n",
    "        ax[a,b].imshow(wordcloud, interpolation='bilinear')\n",
    "        ax[a,b].set(title='\\\"' + biblebooks.axes[0][b*11+a] + '\\\"')\n",
    "        ax[a,b].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(data)\n",
    "ratios = np.cumsum(pca.explained_variance_ratio_)\n",
    "#ratios = temp/(np.arange(len(temp))+1)\n",
    "maxRatioIndex = 0\n",
    "for n in range(1,len(ratios)):\n",
    "    if ratios[n]/(n/len(ratios)+1) > ratios[maxRatioIndex]/(maxRatioIndex/len(ratios)+1):\n",
    "        maxRatioIndex = n\n",
    "#ratios = sorted(ratios)\n",
    "plt.plot (ratios)\n",
    "print (\"The number of principle components with the highest ratio of variance to components is\", maxRatioIndex)\n",
    "print (\"Using\", maxRatioIndex, \"components will preserve\", str(100*ratios[maxRatioIndex].round(4)) + \"% of the data\")\n",
    "plt.show()\n",
    "pca = PCA(n_components=maxRatioIndex)\n",
    "pca.fit(data)\n",
    "dataPCA = pca.transform(data)\n",
    "print (dataPCA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (biblebooks.shape)\n",
    "print (data.shape)\n",
    "bibledata = biblebooks\n",
    "for n , columnLabel in enumerate(data.axes[1].tolist()):\n",
    "    bibledata[columnLabel] = data[columnLabel].tolist()\n",
    "print (bibledata.shape)\n",
    "bibledata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first find optimal number of clusters using silhouette scores\n",
    "silhouetteScores = []\n",
    "for numClusters in range(3,50):\n",
    "    kmeans = KMeans(n_clusters=numClusters, random_state=int(time.time()))\n",
    "    pred = kmeans.fit_predict(data)\n",
    "    silhouetteScores.append([silhouette_score(data, pred),numClusters])\n",
    "optimal = sorted(silhouetteScores)[-1][1]\n",
    "print (\"The optimal number of clusters according to silhouette scores is\", optimal)\n",
    "plt.figure(figsize=(10,10))\n",
    "kmeans = KMeans(n_clusters=optimal, random_state=int(time.time()))\n",
    "kmeans.fit(data)\n",
    "kmeansLabels = kmeans.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first find optimal number of clusters using silhouette scores\n",
    "silhouetteScores = []\n",
    "for numComponents in range(3,50):\n",
    "    gmm = GaussianMixture(n_components=numComponents, covariance_type='full')\n",
    "    gmm.fit(data)\n",
    "    pred = gmm.predict(data)\n",
    "    silhouetteScores.append([silhouette_score(data, pred),numClusters])\n",
    "optimal = sorted(silhouetteScores)[-1][1]\n",
    "print (\"The optimal number of clusters according to silhouette scores is\", optimal)\n",
    "plt.figure(figsize=(10,10))\n",
    "gmm = GaussianMixture(n_components=optimal, covariance_type='full').fit(data)\n",
    "gmmLabels = gmm.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first find optimal number of clusters using silhouette scores\n",
    "silhouetteScores = []\n",
    "for numClusters in range(3,50):\n",
    "    agglo = AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto', connectivity=None, linkage='ward', memory=None, n_clusters=numClusters, pooling_func='deprecated')\n",
    "    pred = agglo.fit_predict(data)\n",
    "    silhouetteScores.append([silhouette_score(data, pred),numClusters])\n",
    "optimal = sorted(silhouetteScores)[-1][1]\n",
    "print (\"The optimal number of clusters according to silhouette scores is\", optimal)\n",
    "plt.figure(figsize=(10,10))\n",
    "agglo = AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto', connectivity=None, linkage='ward', memory=None, n_clusters=optimal, pooling_func='deprecated')\n",
    "agglo.fit(data)\n",
    "aggloLabels = agglo.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first find optimal number of clusters using silhouette scores\n",
    "silhouetteScores = []\n",
    "for numClusters in range(3,50):\n",
    "    spectral = SpectralClustering(n_clusters=numClusters, affinity='nearest_neighbors', assign_labels='kmeans')\n",
    "    pred = spectral.fit_predict(data)\n",
    "    silhouetteScores.append([silhouette_score(data, pred),numClusters])\n",
    "optimal = sorted(silhouetteScores)[-1][1]\n",
    "print (\"The optimal number of clusters according to silhouette scores is\", optimal)\n",
    "plt.figure(figsize=(10,10))\n",
    "spectral = SpectralClustering(n_clusters=optimal, affinity='nearest_neighbors', assign_labels='kmeans')\n",
    "spectral.fit(data)\n",
    "spectralLabels = spectral.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bibledata['kmeansLabels'] = kmeansLabels\n",
    "bibledata['gmmLabels'] = gmmLabels\n",
    "bibledata['aggloLabels'] = aggloLabels\n",
    "bibledata['spectralLabels'] = spectralLabels\n",
    "sections = {m:(n*20+30) for n, m in enumerate(biblebooks['Sections'].unique())}\n",
    "cat = []\n",
    "for n, row in biblebooks.iterrows():\n",
    "    cat.append(row['Testaments'] == 'OT')\n",
    "bibledataOT = bibledata.loc[cat]\n",
    "bibledataNT = bibledata.loc[[not b for b in cat]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10,10))\n",
    "ax[0,0].scatter(data[:, 0], data[:, 1], c=kmeansLabels, marker='o', s=[sections[n] for n in biblebooks['Sections'].tolist()], cmap='viridis')\n",
    "ax[0,0].scatter(data[:, 0], data[:, 1], c=kmeansLabels, marker='*', s=[sections[n] for n in biblebooks['Sections'].tolist()], cmap='viridis')\n",
    "ax[0,0].set(xlabel='PC1', ylabel='PC2', title='KMeans Clustering')\n",
    "ax[0,1].scatter(data[:, 0], data[:, 1], c=gmmLabels, marker='o', s=[sections[n] for n in biblebooks['Sections'].tolist()], cmap='viridis')\n",
    "ax[0,1].scatter(data[:, 0], data[:, 1], c=gmmLabels, marker='*', s=[sections[n] for n in biblebooks['Sections'].tolist()], cmap='viridis')\n",
    "ax[0,1].set(xlabel='PC1', ylabel='PC2', title='Gaussian Mixture Models Clustering')\n",
    "ax[1,0].scatter(data[:, 0], data[:, 1], c=aggloLabels, marker='o', s=[sections[n] for n in biblebooks['Sections'].tolist()], cmap='viridis')\n",
    "ax[1,0].scatter(data[:, 0], data[:, 1], c=aggloLabels, marker='*', s=[sections[n] for n in biblebooks['Sections'].tolist()], cmap='viridis')\n",
    "ax[1,0].set(xlabel='PC1', ylabel='PC2', title='Agglomerative Clustering')\n",
    "ax[1,1].scatter(data[:, 0], data[:, 1], c=spectralLabels, marker='o', s=[sections[n] for n in biblebooks['Sections'].tolist()], cmap='viridis')\n",
    "ax[1,1].scatter(data[:, 0], data[:, 1], c=spectralLabels, marker='*', s=[sections[n] for n in biblebooks['Sections'].tolist()], cmap='viridis')\n",
    "ax[1,1].set(xlabel='PC1', ylabel='PC2', title='Spectral Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. What is the optimal number of clusters of these 66 Books? Find these clusters and describe them. Are you surprised at your finding? Why/Why not? Graph and color your clusters (probably on the first two PC's). On the graph, show your clusters in colors, the Testaments in plot symbols, and the Sections in sizes.\n",
    "\n",
    "    answer A\n",
    "\n",
    "---- i) How do the 2 Testaments fall into your clusters? Tabulate the counts in a table with rows showing Testaments in the given order and columns showing your clusters in the order of total frequencies. \n",
    "\n",
    "    answer Ai\n",
    "\n",
    "---- ii) How do the 7 Sections fall into your clusters? Tabulate the counts in a table with rows showing Sections in the given order and columns showing your clusters in the order of total frequencies.\n",
    "\n",
    "    answer Aii\n",
    "\n",
    "B. How would Association Analyses help to reveal characteristic word clusters? Produce word clouds for the top 10 words clusters with the top 100 most frquent words. Describe these word clusters, and what they are telling you about the Bible. How do these top 10 words clouds represent the 2 Testaments and the 7 Sections?\n",
    "\n",
    "    answer B\n",
    "\n",
    "C. How would Seriation Analyses help to reveal the structure of these 66 Books?\n",
    "\n",
    "    Seriation will put the books in order based on patters in the data. I don't think the books would be put in the same order as in the bible, because this order is fairly arbitrary. It's more likely that the seriation analyses will put the books in order of complexity, or writing style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
